#!/usr/bin/env python3
"""
Curatoria MEGA RUNNER (final, UK-safe, UTF-8)

What it does (end-to-end, audit-friendly):
1) Dataset check (expects public Fashion-MNIST path, but runs in demo mode if missing)
2) Converter (Quantum -> Classical dummy measurements to CSV/JSON)
3) Baseline trainer (dummy multinomial LR-like; writes metrics/model; optional confusion matrix)
4) QML runner (dummy 2-qubit embedding head; writes metrics/model)
5) Reports: COMBINED_REPORT.md, MEGA_REPORT.md, combined_results.json
6) Secret Guard: scans candidate export files for secrets/PII, blocks export if risky
7) Artifact pack: ZIP + .sha256 under exports/
8) GitHub RELEASE_DRAFT.md (both in exports/ and root) for easy publishing

Design goals:
- No personal data processing (UK GDPR-friendly)
- No network calls here (CI may download datasets separately)
- All file writes with encoding='utf-8' to avoid Unicode issues on Pyto/GitHub
"""

from __future__ import annotations
import sys, os, time, json, re, zipfile, hashlib, random
from pathlib import Path
from typing import Dict, List, Tuple

# ---------------- Paths ----------------
WORK = Path.home() / "Documents" / "qml-datasets"
DATA = WORK / "data"
FMNIST = DATA / "fashion-mnist" / "extracted"
CONVERTED = WORK / "converted"
BASELINE = WORK / "baseline"
QML = WORK / "qml"
EXPORTS = WORK / "exports"
for p in [WORK, DATA, FMNIST, CONVERTED, BASELINE, QML, EXPORTS]:
    p.mkdir(parents=True, exist_ok=True)

# ---------------- Utils ----------------
def info(msg: str) -> None: print(f"[INFO] {msg}")
def ok(msg: str) -> None: print(f"[OK] {msg}")
def warn(msg: str) -> None: print(f"[WARN] {msg}")

def write_text_utf8(path: Path, content: str) -> None:
    path.write_text(content, encoding="utf-8")

def sha256sum(path: Path) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1 << 20), b""):
            h.update(chunk)
    return h.hexdigest()

# ---------------- Secret / PII guard ----------------
SECRET_PATTERNS = [
    r"AKIA[0-9A-Z]{16}",                 # AWS Access Key ID
    r"ghp_[0-9A-Za-z]{36}",              # GitHub classic token
    r"-----BEGIN PRIVATE KEY-----",      # PEM private key
    r"\b\d{16}\b",                       # 16-digit number (possible card)
    r"[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}",  # e-mails
]
SECRET_RE = re.compile("|".join(SECRET_PATTERNS), re.IGNORECASE)

def secret_guard(files: List[Path]) -> List[Path]:
    """Return list of risky files. If non-empty, export must be blocked."""
    risky: List[Path] = []
    for p in files:
        try:
            if p.suffix.lower() in {".json", ".txt", ".md", ".yaml", ".yml", ".py", ".csv"}:
                txt = p.read_text(encoding="utf-8", errors="ignore")
                if SECRET_RE.search(txt):
                    risky.append(p)
        except Exception:
            # treat unreadable/binary as safe here (we only scan text-like)
            pass
    return risky

# ---------------- Steps ----------------
def step_check_datasets() -> Tuple[Dict, int]:
    """
    Check presence of Fashion-MNIST .gz files.
    This runner does not download; CI pipeline may do that.
    """
    files = [
        FMNIST / "train-images-idx3-ubyte.gz",
        FMNIST / "train-labels-idx1-ubyte.gz",
        FMNIST / "t10k-images-idx3-ubyte.gz",
        FMNIST / "t10k-labels-idx1-ubyte.gz",
    ]
    present = all(p.exists() for p in files)
    if present:
        ok(f"Fashion-MNIST found at: {FMNIST}")
    else:
        warn("Fashion-MNIST missing or incomplete; proceeding in demo mode.")
    status = {"dataset": "fashion-mnist", "location": str(FMNIST), "present": present}
    return status, 0

def step_converter() -> Dict:
    """
    Produce simple quantum->classical dummy measurements (CSV/JSON).
    Purely synthetic, no personal data.
    """
    CONVERTED.mkdir(parents=True, exist_ok=True)
    rows = []
    for i in range(2):
        rows.append({
            "id": f"dummy_{i+1}",
            "source": "demo",
            "bitstring": random.choice(["00", "01", "10", "11"]),
            "shot_total": random.randint(100, 300),
        })
    jpath = CONVERTED / "measurements.json"
    cpath = CONVERTED / "measurements.csv"
    write_text_utf8(jpath, json.dumps(rows, indent=2))
    header = "id,source,bitstring,shot_total\n"
    body = "".join(f"{r['id']},{r['source']},{r['bitstring']},{r['shot_total']}\n" for r in rows)
    write_text_utf8(cpath, header + body)
    ok(f"Converter wrote: {jpath}, {cpath}")
    return {"converted": True, "records": len(rows), "json": str(jpath), "csv": str(cpath)}

def step_baseline(idx: int) -> Dict:
    """
    Dummy baseline trainer that simulates 3 epochs and writes metrics/model.
    If matplotlib is available, writes a toy confusion matrix image.
    """
    BASELINE.mkdir(parents=True, exist_ok=True)

    # simulate a tiny history
    history = []
    val_acc = 0.60
    for e in [1, 2, 3]:
        loss = 10.0 / (e + 1) + random.random()
        val_acc = max(val_acc + random.uniform(-0.02, 0.03), 0.10)
        history.append({"epoch": e, "loss": round(loss, 4), "val_acc": round(val_acc, 4)})
    test_acc = 0.72  # demo
    metrics = {"test_accuracy": test_acc, "history": history}
    (BASELINE / "metrics.json").write_text(json.dumps(metrics, indent=2), encoding="utf-8")

    # fake model (robust if numpy is missing)
    try:
        import numpy as np
        np.savez(BASELINE / "model.npz", W=np.random.randn(10, 784), b=np.random.randn(10))
    except Exception as e:
        warn(f"Could not save baseline model.npz (numpy missing?): {e}")

    # optional confusion matrix
    try:
        import numpy as np
        import matplotlib
        matplotlib.use("Agg")
        import matplotlib.pyplot as plt
        import itertools
        cm = np.array([[80, 20], [18, 82]])  # toy 2x2
        classes = ["0", "1"]
        fig = plt.figure()
        plt.imshow(cm, interpolation="nearest")
        plt.title("Baseline confusion (demo)")
        tick_marks = range(len(classes))
        plt.xticks(tick_marks, classes)
        plt.yticks(tick_marks, classes)
        fmt = "d"
        thresh = cm.max() / 2.0
        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
            plt.text(j, i, format(cm[i, j], fmt),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        plt.tight_layout()
        plt.ylabel("True")
        plt.xlabel("Pred")
        fig.savefig(BASELINE / "confusion_matrix.png")
        ok("Baseline confusion_matrix.png written.")
    except Exception as e:
        warn(f"Matplotlib not available or failed: {e}")

    ok(f"Baseline metrics/model written to: {BASELINE}")
    return metrics

def step_qml(idx: int) -> Dict:
    """
    Dummy QML runner. Writes small metrics and a toy model file.
    """
    QML.mkdir(parents=True, exist_ok=True)
    train_acc = 0.765 + random.uniform(-0.01, 0.01)
    test_acc = 0.795 + random.uniform(-0.01, 0.01)
    results = {"train_accuracy": round(train_acc, 4),
               "test_accuracy": round(test_acc, 4)}
    (QML / "qml_results.json").write_text(json.dumps(results, indent=2), encoding="utf-8")

    try:
        import numpy as np
        np.savez(QML / "qml_model.npz", W=np.random.randn(3, 3), b=np.random.randn(3))
    except Exception:
        pass

    ok(f"QML results/model written to: {QML}")
    return results

# ---------------- Reports & Packaging ----------------
def write_reports(ds: Dict, conv: Dict, bl: Dict, qres: Dict) -> Tuple[Path, Path, Path]:
    combined = WORK / "COMBINED_REPORT.md"
    mega = WORK / "MEGA_REPORT.md"
    results_json = WORK / "combined_results.json"

    data = {
        "dataset": ds,
        "converter": conv,
        "baseline": bl,
        "qml": qres,
        "generated_utc": time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime()),
    }
    write_text_utf8(results_json, json.dumps(data, indent=2))

    comb_md = [
        "# COMBINED_REPORT.md",
        "",
        "Baseline vs QML (demo run)",
        f"- Baseline test_acc: {bl.get('test_accuracy', 'n/a')}",
        f"- QML test_acc: {qres.get('test_accuracy', 'n/a')}",
        "",
        "Files:",
        f"- Baseline: {BASELINE}",
        f"- QML:      {QML}",
        f"- Converted: {CONVERTED}",
    ]
    write_text_utf8(combined, "\n".join(comb_md))

    mega_md = [
        "# MEGA_REPORT.md",
        "",
        "Extended experiment summary (demo).",
        "Environment:",
        f"- Python: {sys.version.split()[0]}",
        f"- Platform: {sys.platform}",
        "",
        "Dataset status:",
        json.dumps(ds, indent=2),
        "",
        "Metrics:",
        f"- Baseline: {json.dumps(bl, indent=2)}",
        f"- QML:      {json.dumps(qres, indent=2)}",
    ]
    write_text_utf8(mega, "\n".join(mega_md))

    # Mirror copies to EXPORTS for packaging convenience
    write_text_utf8(EXPORTS / "COMBINED_REPORT.md", combined.read_text(encoding="utf-8"))
    write_text_utf8(EXPORTS / "MEGA_REPORT.md", mega.read_text(encoding="utf-8"))
    write_text_utf8(EXPORTS / "combined_results.json", results_json.read_text(encoding="utf-8"))

    ok(f"Reports written: {combined}, {mega}, {results_json}")
    return combined, mega, results_json

def collect_for_export() -> List[Path]:
    candidates = [
        EXPORTS / "COMBINED_REPORT.md",
        EXPORTS / "MEGA_REPORT.md",
        EXPORTS / "combined_results.json",
        BASELINE / "metrics.json",
        BASELINE / "model.npz",
        BASELINE / "confusion_matrix.png",
        QML / "qml_results.json",
        QML / "qml_model.npz",
    ]
    return [p for p in candidates if p.exists()]

def pack_artifact(files: List[Path]) -> Tuple[Path, Path]:
    risky = secret_guard(files)
    if risky:
        warn("Export blocked! Risky content found in the following files:")
        for r in risky:
            print("  -", r)
        sys.exit(3)

    ts = time.strftime("%Y%m%d_%H%M%S", time.gmtime())
    zip_path = EXPORTS / f"curatoria_artifact_EXT_{ts}.zip"
    sha_path = EXPORTS / f"curatoria_artifact_EXT_{ts}.zip.sha256"

    with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
        for f in files:
            zf.write(f, arcname=f.name)
    sha = sha256sum(zip_path)
    write_text_utf8(sha_path, f"{sha}  {zip_path.name}")
    ok(f"Artifact: {zip_path}")
    ok(f"SHA256 : {sha_path}")
    return zip_path, sha_path

def write_release_draft(zip_path: Path) -> Tuple[Path, Path]:
    md = f"""# GitHub Release Draft (Curatoria)

Tag: v0.1.0
Release title: Curatoria Quantum ML - First Artifact Release

## Included Artifact
- {zip_path.name}
- {zip_path.name}.sha256

## Contents of the ZIP
- COMBINED_REPORT.md  -> Baseline + QML side-by-side
- MEGA_REPORT.md      -> Extended comparison & environment
- combined_results.json -> Machine-readable metrics
- baseline/*          -> Classical baseline model + metrics
- qml/*               -> Quantum ML model + metrics
- converted/*         -> Converted data (Q->C)

## Verification
1) Download the ZIP and .sha256 file.
2) Run:  shasum -a 256 -c {zip_path.name}.sha256
3) If it prints OK, the artifact is verified.

(Generated UTC: {time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime())})
"""
    draft_exports = EXPORTS / "RELEASE_DRAFT.md"
    draft_root = WORK / "RELEASE_DRAFT.md"
    write_text_utf8(draft_exports, md)
    write_text_utf8(draft_root, md)
    ok(f"Release draft written: {draft_exports}")
    return draft_root, draft_exports

# ---------------- Main ----------------
def main() -> None:
    info("=== Curatoria MEGA RUNNER (UK-safe, UTF-8) ===")
    ds_status, idx = step_check_datasets()
    conv = step_converter()
    bl = step_baseline(idx)
    qres = step_qml(idx)

    rep_comb, rep_mega, rep_json = write_reports(ds_status, conv, bl, qres)
    files = collect_for_export()
    zip_path, sha_path = pack_artifact(files)
    draft_root, draft_exports = write_release_draft(zip_path)

    print("\n=== Mega Summary ===")
    print(f"- COMBINED_REPORT : {rep_comb}")
    print(f"- MEGA_REPORT     : {rep_mega}")
    print(f"- Results JSON    : {rep_json}")
    print(f"- ZIP             : {zip_path}")
    print(f"- SHA256          : {sha_path}")
    print(f"- RELEASE_DRAFT   : {draft_exports}\n")

if __name__ == "__main__":
    main()
